{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3.1 - Introduction to virtual environments and using packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will briefly touch upon the idea of packages and virtual environments, before we get hands-on with some data analysis packages such as Pandas, Numpy & Matpplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonly used libraries in the Standard Library\n",
    "\n",
    "The Python Standard Library is quite extensive and consists of more than 200 core modules. These modules cover a wide range of functionalities, from file I/O and system calls, to internet protocols - some more useful, or more commonly used, than others - the beauty of them all being that they are installed when Python is installed!\n",
    "\n",
    "Below we'll see a basic, example of some of the libraries that are commonly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules from the Python Standard Library\n",
    "import datetime\n",
    "import os\n",
    "import platform\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Get the current platform\n",
    "current_platform = platform.system()\n",
    "\n",
    "# Generate a random number\n",
    "random_number = random.randint(1, 10)\n",
    "\n",
    "# Calculate the square root of the random number\n",
    "sqrt = math.sqrt(random_number)\n",
    "\n",
    "# Create a dictionary with the results\n",
    "results = {\n",
    "    \"date_time\": str(now),\n",
    "    \"directory\": cwd,\n",
    "    \"platform\": current_platform,\n",
    "    \"random_number\": random_number,\n",
    "    \"square_root\": sqrt\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a JSON string\n",
    "json_results = json.dumps(results, indent=4)\n",
    "\n",
    "# Print the JSON string\n",
    "print(\"Hello, here are some fun facts for you in JSON format:\")\n",
    "print(json_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual environments\n",
    "\n",
    "Now Azure ML has a few conda environments already created for us... head over to the terminal of your running compute instance and run `conda env list` to see the full list. As we covered previously, each of these virtual environments will have a collection of different packages and versions of Python (some even slated as end-of-life...)\n",
    "\n",
    "\n",
    "To change your current environment, you can run the command `conda activate <insert_env_name_here>` which will 'deactivate' the current environment and then 'activate' the new environment. Once this has completed, go ahead and run the following command `conda list` - this will print all of the packages installed within the environment you have just activated.\n",
    "\n",
    "Now to create a new conda environment... The command `conda create -n myenv python=3.9 scipy=0.17.3 astroid babel` will create a new environment with the name 'myenv', with Python v3.9 and a few packages installed. Note that after creating the environment, you will need to run the 'activate' command in order to use it.\n",
    "\n",
    "Make sense?! With the fundaments covered, you can now go ahead and create a new virtual environment with Python v3.11 and the Pandas library..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that has been completed, we have a few more steps to follow...\n",
    "\n",
    "When we run our notebooks, we use one of the kernels found in the top right of the notebook - but how do can we use our newly created virtual environment in a notebook?\n",
    "\n",
    "A kernel, while very similar to a virtual environment, and can even be associated with a specific virtual environment, is used as a computational engine specifically to run cells within a notebook. To convert our virtual environment into a kernel, we can run the following commands:\n",
    "\n",
    "`pip install ipykernel`\n",
    "\n",
    "`python -m ipykernel install --user --name <env_name_here> --display-name \"<kernel_name_here>\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our newly created kernel for some data handling and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the functions used in the  the latter part of this notebook will be described with the appropriate comments, but VS Code offers a feature whereby you can hover your cursor over a function and it will describe it's behaviour! I have also created a wiki describing all the functions used in this notebook - see `function-wiki.md`. As a last resort, I advice looking through the difference library documentations found below:\n",
    "\n",
    "* Pandas : https://pandas.pydata.org/docs/\n",
    "* Matplotlib : https://matplotlib.org/stable/index.html\n",
    "* NumPy : https://numpy.org/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages - ensure you have chosen your correct kernel before running this cell!\n",
    "# Note: this is not necessary if you have installed the packages when creating the virtual environment!\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Import pandas with alias pd, so it is easier to use and refer to.\n",
    "import numpy as np # Import numpy with alias np, so it is easier to use and refer to.\n",
    "import matplotlib # Import whole package of matplotlib in order to get version number. Bad practice, see further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "\n",
    "import matplotlib.pyplot as plt # Import only pyplot from matplotlib. Good practice, this way you know where the function comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example exploratory data analysis\n",
    "\n",
    "This section will explore some basic data analysis on a dataset containing order history found in the `data.csv` file. Please note that this dataset is randomly generated and contains no real-world data or personal information.\n",
    "\n",
    "We will first interrogate the data to uncover trends before plotting informative graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling with Pandas\n",
    "\n",
    "As covered in the session, pandas is a powerful Python library primarily used for data manipulation and analysis. It provides data structures and functions needed to manipulate structured data, including functions for reading and writing data in a wide variety of formats. With Pandas, you can filter and sort data, perform statistical analysis, and apply data transformations. It's particularly useful for working with numerical tables or time series data. Use-cases include cleaning data, transforming data, visualising data, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data/data.csv\") # Read in the data using pandas read_csv function. The data is stored in a pandas DataFrame.\n",
    "\n",
    "raw_data.head() # Show the first 5 rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns # Show the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['order_id'] # Show the column with the name 'order_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.dtypes # Show the data types of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First off, let's convert all the columns to the correct data type.\n",
    "\n",
    "raw_data_ = raw_data.copy() # Make a copy of the data so we don't overwrite the original.\n",
    "raw_data_['order_id'] = raw_data['order_id'].apply(lambda x: str(x)) # Convert order_id to string.\n",
    "raw_data_['order_cost'] = raw_data['order_cost'].apply(lambda x: x[1:]).astype('float64') # Convert order_cost to float, removing the $ sign first using string slicing.\n",
    "raw_data_['country'] = raw_data['country'].astype('category') # Convert country to category.\n",
    "raw_data_['email'] = raw_data['email'].apply(lambda x: str(x)) # Convert order_id to string.\n",
    "\n",
    "raw_data_.dtypes # Check the data types again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As is best practice, we should check for missing values as they need to be addressed before we can do any analysis.\n",
    "\n",
    "raw_data_.replace('nan', np.nan, inplace=True) # Replace all 'nan' strings with np.nan, best practice for handling missing values as lots of pandas functions can handle np.nan.\n",
    "for col in raw_data_.columns: # Loop through each column.\n",
    "    print(f\"{col}: {raw_data_[col].isna().sum()}\") # Check for missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_.loc[raw_data_['email'].isna(), 'email'] # Check the rows where email is NaN using pd.loc for a conditional selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_.iloc[[19, 35, 89, 96]] # Check the rows where email is 'nan' using pd.iloc for index reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = raw_data_.drop(index=[19, 35, 89, 96]) # Drop the rows where email is 'nan' using the index reference from above.\n",
    "clean_data = clean_data.reset_index(drop=True) # Reset the index so it is continuous, with no gaps after dropping rows. We now have a clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have cleaned the data, we can start some basic analysis.\n",
    "\n",
    "clean_data.describe(include='all') # Show basic some descriptive statistics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the minimum and maximum order cost.\n",
    "\n",
    "max_order = clean_data['order_cost'].min() # Find the minimum value in the column\n",
    "min_order = clean_data['order_cost'].max() # Find the maximum value in the column\n",
    "\n",
    "print(f\"Max order: ${max_order}\")\n",
    "print(f\"Min order: ${min_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the distrubition of deal categories.\n",
    "\n",
    "deal_cat_count = clean_data['deal_category'].value_counts() # Count the number of each deal category\n",
    "print(deal_cat_count) # Print the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the average order cost for each deal category.\n",
    "\n",
    "deal_cat_mean = clean_data.groupby('deal_category')['order_cost'].mean() # Group the data by deal category, then find the mean of order cost for each group\n",
    "print(deal_cat_mean) # Print the means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see each countries single, most common deal category.\n",
    "\n",
    "grouped_deal_cat = clean_data.groupby('country')['deal_category'] # Group the data by country\n",
    "country_deal_cat = grouped_deal_cat.apply(lambda x: x.value_counts().index[0]) # Use a lambda function to find the most common deal category for each country\n",
    "\n",
    "print(country_deal_cat) # Print the most common deal category for each country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualisation with Matplotlib\n",
    "\n",
    "Matplotlib is a powerful Python library used for creating static, animated, and interactive visualisations in Python. It provides an object-oriented API for embedding plots into applications. With Matplotlib, you can generate line plots, scatter plots, bar charts, error charts, histograms, pie charts, and much more with just a few lines of code. It's a great tool for turning complex data into easily understandable visuals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's continue on and explore if there is a relationship between order cost and deal category.\n",
    "\n",
    "plt.bar(clean_data['deal_category'].unique(), deal_cat_mean) # Plot the mean order cost for each category in a bar chart.\n",
    "plt.xlabel('Deal Category') # Label the x axis.\n",
    "plt.ylabel('Mean Order Cost') # Label the y axis.\n",
    "plt.title('Mean Order Cost by Deal Category') # Title the plot.\n",
    "plt.show() # Show the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the correlation between order cost and deal category using numpy's corrcoef function.\n",
    "# Note that correlation does not imply causation and is a very basic test of the relationship between two variables.\n",
    "# For more information on correlation, see https://en.wikipedia.org/wiki/Correlation_and_dependence.\n",
    "# A value of 0 means no correlation, a value of 1 means perfect correlation, and a value of -1 means perfect negative correlation.\n",
    "\n",
    "corr_value = np.corrcoef(clean_data['order_cost'], clean_data['deal_category']) # Find the correlation coefficient between order cost and deal category.\n",
    "print(f\"Correlation coefficient: {corr_value[0, 1]}\") # Print the correlation coefficient, note that the corrcoef function returns a 2x2 matrix, so we need to index the value we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a histogram of the order_cost column, helping us to visualise the distribution of values across the orders.\n",
    "\n",
    "plt.hist(clean_data['order_cost'], bins=20) # Plot a histogram of order_cost with 20 bins.\n",
    "plt.xlabel('Order Cost ($)') # Set the x-axis label.\n",
    "plt.ylabel('Frequency') # Set the y-axis label.\n",
    "plt.xticks(rotation=45) # Rotate the x-axis labels by 45 degrees.\n",
    "plt.xticks(np.arange(0, 100, 5)) # Set the x-axis ticks to be every 5 dollars, as we know the range of values is between 0 and 100 from our previous analysis.\n",
    "plt.show() # Show the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's explore the relationship between order_cost and country. \n",
    "# To being let's look at the number of orders from each country.\n",
    "\n",
    "num_orders_country = clean_data['country'].value_counts() # Count the number of orders from each country.\n",
    "plt.bar(num_orders_country.index, num_orders_country) # Plot a bar chart of the number of orders from each country.\n",
    "plt.xlabel('Country') # Set the x-axis label\n",
    "plt.ylabel('Number of orders') # Set the y-axis label\n",
    "plt.xticks(rotation=90) # Rotate the x-axis labels by 90 degrees\n",
    "plt.title('Number of orders by country') # Set the title\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_order_per_country = clean_data.groupby('country')['order_cost'].mean() # Find the mean order_cost for each country\n",
    "plt.bar(mean_order_per_country.index, mean_order_per_country) # Plot a bar chart of the mean order_cost for each country\n",
    "plt.xlabel('Country') # Set the x-axis label\n",
    "plt.ylabel('Mean Order Cost ($)') # Set the y-axis label\n",
    "plt.xticks(rotation=90) # Rotate the x-axis labels by 90 degrees\n",
    "plt.subplots_adjust(bottom=0.4) # Adjust the bottom of the plot to make room for the x-axis labels\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view the distribution of order_cost by country using a boxplot.\n",
    "# A box plot shows the distribution of values for each category, as well as the median, interquartile range and outliers.\n",
    "# In the below plot, the box represents the interquartile range, the line in the middle of the box is the median, the whiskers represent the range of values, and the dots are outliers.\n",
    "# As some countries have a lot more orders than others, it is difficult to see the distribution of values for each country.\n",
    "\n",
    "clean_data.boxplot(column='order_cost', by='country') # Plot a boxplot of order_cost by country\n",
    "plt.ylabel('Order Cost ($)') # Set the y-axis label\n",
    "plt.xlabel('Country') # Set the x-axis label\n",
    "plt.xticks(rotation=90) # Rotate the x-axis labels by 90 degrees\n",
    "plt.subplots_adjust(bottom=0.4) # Adjust the bottom of the plot to make room for the x-axis labels\n",
    "plt.suptitle('') # Remove the default super-title\n",
    "plt.title('Boxplot of Order Cost by Country') # Set the title of the plot\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a boxplot of order_cost by country, but only for the top 5 countries by number of orders.\n",
    "# This will allow us to see the distribution of values for each country more clearly, as the number of orders is more similar between countries.\n",
    "\n",
    "top_5_countries = clean_data['country'].value_counts().index[:5] # Get the top 5 countries by number of orders using the index of the value_counts series and slicing.\n",
    "clean_data_top_5 = clean_data.loc[clean_data['country'].isin(top_5_countries)] # Filter the data to only include the top 5 countries using pd.loc for a conditional selection and the isin function.\n",
    "clean_data_top_5.boxplot(column='order_cost', by='country') # Plot a boxplot of order_cost by country.\n",
    "plt.ylabel('Order Cost ($)') # Set the y-axis label.\n",
    "plt.xlabel('Country') # Set the x-axis label.\n",
    "plt.xticks(rotation=90) # Rotate the x-axis labels by 90 degrees.\n",
    "plt.subplots_adjust(bottom=0.4) # Adjust the bottom of the plot to make room for the x-axis labels.\n",
    "plt.suptitle('') # Remove the default super-title.\n",
    "plt.title('Boxplot of Order Cost by Country (Top 5)') # Set the title of the plot.\n",
    "plt.show() # Show the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework challenge\n",
    "\n",
    "This week's homework will use one of the most common datasets in the machine learning space, the Iris dataset. The Iris dataset is a collection of 150 measurements of three species of iris flowers: setosa, versicolor, and virginica. Each measurement includes the length and width of the sepal and petal of a flower in centimeters. The dataset is widely used as an example of multivariate analysis and machine learning techniques.\n",
    "\n",
    "Your task is to perform some exploratory data analysis on the Iris dataset, perhaps exploring the relationship between each feature and creating relevant plots to help tell the story! How you choose to interrogate the data is up to you; use the previous notebook to help you decide on the different ways you could!\n",
    "\n",
    "Use the following code to load the data into the Pandas DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "raw_data = pd.read_csv('https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv') # Read data from URL\n",
    "\n",
    "print(raw_data.head()) # print first 5 rows of data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALE_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
